{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hallucination Score Analysis\n",
    "\n",
    "In this notebook, the hallucination scores of the methods are analyzed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "import json\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import binarize\n",
    "from scipy.stats import chi2_contingency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING\n",
    "# ______________\n",
    "# load the dataframes and durations from the results folder\n",
    "def load_dataframes(f):\n",
    "    dataset_name = f.split(\"__\")[0].split(\"_\")[-4]\n",
    "    model_name = f.split(dataset_name)[0].split(\"\\\\\")[-1] +\"_\"+ f.split(\"__\")[-1].split(\".\")[0]\n",
    "    df = pd.read_csv(f)\n",
    "    df.drop([col for col in df.columns if \"answer\" in col or \"prompt\" in col], axis=1, inplace=True)\n",
    "    durations = f.split(dataset_name)[0] + \"durations.csv\"\n",
    "    durations_df = pd.read_csv(durations) if len(glob.glob(durations)) > 0 else None\n",
    "    return {\"dataset_name\": dataset_name, \"model\": model_name, \"df\": df, \"durations\": durations_df}\n",
    "\n",
    "\n",
    "# PREPROCESSING\n",
    "# ______________\n",
    "# fix json errors in the column lbhd_score\n",
    "def fix_json_errors(df):\n",
    "    count = 0\n",
    "    for index, row in df.iterrows():\n",
    "        json_str = row['lbhd_score'].replace(\"'\", \"\\\"\")\n",
    "        while True:\n",
    "            try:\n",
    "                # Attempt to parse the JSON string\n",
    "                json.loads(json_str)\n",
    "                break  # Exit the loop if no error\n",
    "            except json.JSONDecodeError as e:\n",
    "                # If there's a JSONDecodeError, extract the position of the error\n",
    "                error_pos = e.pos\n",
    "                # print(f\"Fixing error in row {index} at position: {error_pos}\")\n",
    "                # Add a backslash to the string in front of the error to escape the character\n",
    "                json_str = json_str[:error_pos - 1] + \"\\\\\" + json_str[error_pos - 1:]\n",
    "                # Replace the JSON string in the DataFrame with the corrected one\n",
    "                df.at[index, 'lbhd_score'] = json_str\n",
    "                count += 1\n",
    "    print(f\"Fixed {count} JSON errors\\n\")\n",
    "    return df\n",
    "\n",
    "# normalize the columns\n",
    "def normalize_column(column):\n",
    "    # Apply Min-Max normalization\n",
    "    column = (column - column.min()) / (column.max() - column.min())\n",
    "    # rescale to [0, 1] range (from [-1, 1] range)\n",
    "    column = (column + 1) / 2\n",
    "    return column\n",
    "\n",
    "# process the lbhd_score column\n",
    "def process_lbhd_scores(df):\n",
    "    # Initialize new columns with default values\n",
    "    df['lbhd_sent_avg'] = 0.0\n",
    "    df['lbhd_sent_min'] = 0.0\n",
    "    df['lbhd_sent_normalized_product'] = 0.0\n",
    "    df['lbhd_concept_avg'] = 0.0\n",
    "    df['lbhd_concept_min'] = 0.0\n",
    "    df['lbhd_concept_normalized_product'] = 0.0\n",
    "    # Iterate over each row to process the 'lbhd_score' column\n",
    "    for index, row in df.iterrows():\n",
    "        # Parse the string representation of the dictionary into an actual dictionary\n",
    "        lbhd_score = json.loads(row['lbhd_score'].replace(\"'\", \"\\\"\"))        \n",
    "        # Extract the low-level scores\n",
    "        df.at[index, 'lbhd_sent_avg'] = lbhd_score['avg']\n",
    "        df.at[index, 'lbhd_sent_min'] = lbhd_score['min']\n",
    "        df.at[index, 'lbhd_sent_normalized_product'] = lbhd_score['normalized_product']        \n",
    "        # Aggregate the concept-level scores\n",
    "        concept_avgs = [concept[next(iter(concept))]['avg'] for concept in lbhd_score['concepts']]\n",
    "        concept_mins = [concept[next(iter(concept))]['min'] for concept in lbhd_score['concepts']]\n",
    "        concept_norm_prods = [concept[next(iter(concept))]['normalized_product'] for concept in lbhd_score['concepts']]        \n",
    "        df.at[index, 'lbhd_concept_avg'] = np.mean(concept_avgs)\n",
    "        df.at[index, 'lbhd_concept_min'] = np.min(concept_mins)\n",
    "        def normalized_product(probabilities):\n",
    "            adjusted_probs = [max(p, 1e-30) for p in probabilities]\n",
    "            log_probs = np.log(adjusted_probs)\n",
    "            score = np.exp(np.sum(log_probs) / len(adjusted_probs))\n",
    "            return score\n",
    "        df.at[index, 'lbhd_concept_normalized_product'] = normalized_product(concept_norm_prods)\n",
    "    # Remove the 'lbhd_score' column\n",
    "    df.drop('lbhd_score', axis=1, inplace=True)\n",
    "    # Normalize the new columns\n",
    "    df['lbhd_sent_avg'] = normalize_column(df['lbhd_sent_avg'])\n",
    "    df['lbhd_sent_min'] = normalize_column(df['lbhd_sent_min'])\n",
    "    df['lbhd_sent_normalized_product'] = normalize_column(df['lbhd_sent_normalized_product'])\n",
    "    df['lbhd_concept_avg'] = normalize_column(df['lbhd_concept_avg'])\n",
    "    df['lbhd_concept_min'] = normalize_column(df['lbhd_concept_min'])\n",
    "    df['lbhd_concept_normalized_product'] = normalize_column(df['lbhd_concept_normalized_product'])\n",
    "    # reverse calculation of: linear_probs = [(prob + 1) / 2 for prob in linear_probs]\n",
    "    for col in df.columns:\n",
    "        if \"lbhd\" in col:\n",
    "            df[col] = (df[col] * 2) - 1\n",
    "    return df\n",
    "\n",
    "# Remove rows with errors\n",
    "def remove_err_rows(df):\n",
    "    initial_rows = len(df)\n",
    "\n",
    "\n",
    "# EVALUATION\n",
    "# ______________\n",
    "# calculate the metrics (AUC-ROC, AUC-PR, Precision, Recall, F1) for each column\n",
    "def calculate_metrics(df):\n",
    "    metrics = {}\n",
    "    for column in df.columns:\n",
    "        ground_truth = df['ground_truth'].values\n",
    "        score = df[column].values\n",
    "        # Calculate AUC-ROC\n",
    "        auc_roc = roc_auc_score(ground_truth, score)\n",
    "        # Calculate AUC-PR\n",
    "        auc_pr = average_precision_score(ground_truth, score)\n",
    "        # Binarize predictions based on a threshold (e.g., 0.5)\n",
    "        threshold = 0.9\n",
    "        binarized_score = binarize(score.reshape(-1, 1), threshold=threshold).reshape(-1)\n",
    "        # Calculate Precision, Recall, F1, and F2 scores\n",
    "        precision = precision_score(ground_truth, binarized_score)\n",
    "        recall = recall_score(ground_truth, binarized_score)\n",
    "        f1 = f1_score(ground_truth, binarized_score)\n",
    "        metrics[column] = {\"AUC-ROC\": auc_roc, \"AUC-PR\": auc_pr, \"Precision\": precision, \"Recall\": recall, \"F1\": f1}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# VISUALIZATION\n",
    "# ______________\n",
    "# plot the correlation matrix\n",
    "def plot_correlation_matrix(df, model_name, dataset_name):\n",
    "    # Ensure 'ground_truth' is the last column\n",
    "    columns = [col for col in df.columns if col != 'ground_truth'] + ['ground_truth']\n",
    "    df = df[columns]\n",
    "    # Compute the correlation matrix\n",
    "    corr = df.corr()\n",
    "    # Identify the 3 strongest correlations with 'ground_truth' by absolute value, excluding itself\n",
    "    strongest_corr = corr['ground_truth'].drop('ground_truth').abs().nlargest(3).index.tolist()\n",
    "    fig, ax = plt.subplots(figsize=(11, 9))\n",
    "    cmap = \"YlGnBu\"\n",
    "    # Draw the heatmap with the correct aspect ratio\n",
    "    sns.heatmap(corr, cmap=cmap, vmax=1, vmin=-1, center=0,\n",
    "                square=True, linewidths=.5, cbar_kws={\"shrink\": .5},\n",
    "                annot=True, fmt=\".2f\", annot_kws={\"size\": 7},\n",
    "                mask=np.triu(np.ones_like(corr, dtype=bool)))  # Mask for the upper triangle\n",
    "    # Highlight the columns and rows of the 3 strongest correlations\n",
    "    for strong_corr in strongest_corr:\n",
    "        if not np.tril(np.ones_like(corr, dtype=bool))[df.columns.get_loc(strong_corr), len(df.columns) - 1]:\n",
    "            ax.add_patch(Rectangle((df.columns.get_loc(strong_corr), len(df.columns) - 1), 1, 1, fill=False, edgecolor='red', lw=3))\n",
    "        if not np.tril(np.ones_like(corr, dtype=bool))[len(df.columns) - 1, df.columns.get_loc(strong_corr)]:\n",
    "            ax.add_patch(Rectangle((len(df.columns) - 1, df.columns.get_loc(strong_corr)), 1, 1, fill=False, edgecolor='red', lw=3))\n",
    "    ax.set_xticks(np.arange(len(corr.columns)) + .5)\n",
    "    ax.set_yticks(np.arange(len(corr.columns)) + .5)\n",
    "    ax.set_xticklabels(corr.columns, rotation=45, ha=\"right\")\n",
    "    ax.set_yticklabels(corr.columns)\n",
    "    # Set title with model and dataset names\n",
    "    dataset_name = \"Natural Questions\" if dataset_name == \"nqopen\" else \"XSum\" if dataset_name == \"xsum\" else dataset_name\n",
    "    plt.title(f'Correlation Matrix for {model_name}\\non dataset {dataset_name} with Highlights', fontsize=16, pad=20)\n",
    "    # Adjust layout to fit everything\n",
    "    plt.tight_layout()\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "def plot_categorical(df):\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    if 'ground_truth' in cat_cols:\n",
    "        for col in cat_cols:\n",
    "            if col != 'ground_truth':\n",
    "                tab = pd.crosstab(df[col], df['ground_truth'])\n",
    "                chi2, p, dof, ex = chi2_contingency(tab)\n",
    "                if p < 0.01:\n",
    "                    plt.figure(figsize=(10, 6))\n",
    "                    sns.barplot(x=tab.index, y=tab['ground_truth'], data=tab)\n",
    "                    plt.title(f'{col} vs ground_truth (Chisq P = {p:.2e})')\n",
    "                    plt.xlabel(f'{col}')\n",
    "                    plt.ylabel('Proportion')\n",
    "                    plt.xticks(rotation=90)\n",
    "                    plt.show()\n",
    "    else:\n",
    "        print(\"There is no 'ground_truth' categorical variable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load datasets\n",
    "# find all csvs with scores and ground truths\n",
    "files = glob.glob(\"results/*with_ground_truths*.csv\")\n",
    "\n",
    "# load all csvs into a list of dataframes\n",
    "datasets = [load_dataframes(f) for f in files]\n",
    "\n",
    "print(f\"Found {len(datasets)} CSVs with scores and ground truths for analysis:\\n\")\n",
    "for dataset in datasets:\n",
    "    print(f\"Dataset: {dataset['dataset_name']}\")\n",
    "    print(f\"LLM: {dataset['model']}\")\n",
    "    print(f\"Shape: {dataset['df'].shape}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = datasets[0][\"df\"]\n",
    "# print summary of the dataframe\n",
    "print(test_df.describe())\n",
    "test_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows with errors\n",
    "test_df = remove_err_rows(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process LBHD scores\n",
    "test_df = fix_json_errors(test_df)\n",
    "test_df = process_lbhd_scores(test_df)   # aggregate concept-level scores and normalize (min-max)\n",
    "test_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_matrix(test_df, datasets[0][\"model\"], datasets[0][\"dataset_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for the 'lbhd_concept_min' column\n",
    "metrics = calculate_metrics(test_df)\n",
    "\n",
    "\n",
    "print(f\"Precision: {metrics['lbhd_concept_min']['Precision']}\")\n",
    "print(f\"Recall: {metrics['lbhd_concept_min']['Recall']}\")\n",
    "print(f\"F1: {metrics['lbhd_concept_min']['F1']}\")\n",
    "print(f\"AUC-ROC: {metrics['lbhd_concept_min']['AUC-ROC']}\")\n",
    "print(f\"AUC-PR: {metrics['lbhd_concept_min']['AUC-PR']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at lbhf_concept_min: distribution of scores\n",
    "sns.histplot(test_df['lbhd_concept_min'], kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_categorical(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hdetect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
